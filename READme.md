

# Dimensionality Reduction for Sentiment Analysis with RoBERTa

## ğŸ“Œ Overview

This project leverages **RoBERTa** (*A Robustly Optimized BERT Pretraining Approach*) for sentiment analysis.
It processes tokenized text sequences to classify sentiment, using **contextual embeddings** for richer semantic understanding and improved accuracy.

Our pipeline includes:

* **Feature formatting** for RoBERTa inputs
* **Data preprocessing** (tokenization, padding, attention masks)
* **Model architecture & training strategy**
* **Performance evaluation** with misclassification insights
* **Domain adaptation** for hotel reviews
* **Computational trade-off analysis** vs. simpler models

---

## ğŸ“‚ Feature Format

Input features are tokenized sequences produced by the RoBERTa tokenizer:

* **Tokenization** â†’ Splits text into subword units, handling punctuation, casing, and special characters.
* **Padding** â†’ Uniform sequence length of **512 tokens**.
* **Attention Masks** â†’ Flags real tokens vs. padding.
* **Embedding Conversion** â†’ Converts tokens & masks into dense vector representations.

---

## ğŸ›  Data Preprocessing Steps

1. **Tokenization** â€“ Using Hugging Face RoBERTa tokenizer
2. **Padding/Truncation** â€“ Fixed length = 512 tokens
3. **Attention Mask Generation** â€“ Distinguish real tokens from padding
4. **Embedding Preparation** â€“ Convert to model-ready tensors

---

## ğŸ¤– Model

We use **RoBERTa-base** from Hugging Face.

**Key differences from BERT:**

* **No NSP** â†’ Only Masked Language Modeling (MLM)
* **Dynamic Masking** â†’ New tokens masked every epoch
* **Enhanced Pretraining** â†’ Larger corpus + tuned hyperparameters

---

## ğŸ— Model Architecture

**Input Layers:**

* **Input IDs** â†’ Shape: `(1623, 512)`
* **Attention Masks** â†’ Shape: `(1623, 512)`

**Processing Layers:**

* **RoBERTa Encoder** â†’ Generates contextual embeddings
* **Dropout** â†’ `rate=0.1` to reduce overfitting

**Output Layer:**

* Single neuron with **sigmoid activation** (binary classification)

| Component       | Shape       |
| --------------- | ----------- |
| Input IDs       | (1623, 512) |
| Attention Masks | (1623, 512) |
| Output          | (1623, 1)   |

![RoBERTa Model Architecture]
<img width="3600" height="900" alt="image" src="https://github.com/user-attachments/assets/3c7a8a44-eb70-46d7-a75c-a7e175d34735" />


---

## âš™ Training Strategy

* **Optimizer** â†’ Adam
* **Learning Rate** â†’ `1e-5` (avoids underfitting seen with `1e-4`)
* **Batch Size** â†’ 8 (GPU memory-friendly)
* **Epochs** â†’ 4 (prevents overfitting)
* **Loss Function** â†’ Binary Cross-Entropy (BCE)
* **Class Balancing** â†’ Undersampling to 2,164 samples per class

---

## ğŸ“Š Results Analysis

**Strengths:**

* Strong contextual understanding
* Accurately detects sentiment polarity

**Weaknesses:**

* Struggles with sarcasm, irony, mixed sentiments
* Can misclassify when both pros and cons are present

---

## ğŸ”„ Domain Adaptation for Hotel Reviews

**Approaches:**

1. **Transfer Learning** â€“ Apply product-review-trained model directly to hotel reviews; use LIME for explainability.
2. **Multi-Class Classification** â€“ Map 1â€“10 rating scale using **softmax** and categorical cross-entropy.

---

## ğŸ§  Improving Robustness

* **Ensemble Methods** â€“ Combine multiple RoBERTa variants
* **Data Augmentation** â€“ Paraphrasing, synonym replacement, back-translation
* **Bootstrapping** â€“ Train iteratively on weakly labeled + predicted data
* **Active Learning** â€“ Label most informative samples first

---

## âš– Computational Trade-offs

| Approach       | Accuracy | Contextual Awareness | Compute Cost |
| -------------- | -------- | -------------------- | ------------ |
| **RoBERTa**    | âœ… High   | âœ… Strong             | âŒ High       |
| **TF-IDF/BOW** | âŒ Lower  | âŒ Weak               | âœ… Low        |

